{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b801fb",
   "metadata": {},
   "source": [
    "# Introduction to Generative Modelling\n",
    "\n",
    "*Last updated: **2026-1-13***  \n",
    "\n",
    "This notebook is an **introductory** overview of what modern generative models can do (text, images, audio, video, and multimodal), with a focus on **recent state-of-the-art (SOTA) systems** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95c5f1",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "**Text, figures, and explanations:**  \n",
    "© 2026 Imran Zualkernan. Licensed under **CC BY 4.0**.\n",
    "\n",
    "**Code cells:**  \n",
    "© 2026 Imran Zualkernan. Licensed under the **MIT License**.\n",
    "\n",
    "You are free to reuse, modify, and redistribute with attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa49350",
   "metadata": {},
   "source": [
    "## What is a “generative model”?\n",
    "\n",
    "A **generative model** learns a probability distribution over data (or a procedure that *behaves like* sampling from such a distribution).  \n",
    "\n",
    "Once trained, a **generative model** can **generate** new samples that resemble the training data, conditioned on prompts like:\n",
    "- text (“a photorealistic drone shot of a coral reef”),\n",
    "- an image (“edit this photo to look like golden hour”),\n",
    "- audio (“read this paragraph in a calm voice”),\n",
    "- video (“make a 10-second clip of…”),\n",
    "- or combinations (multimodal prompts).\n",
    "\n",
    "### A useful mental model\n",
    "- **Training:** learn *patterns* in massive datasets.\n",
    "- **Generation:** produce new content that follows the learned patterns **and** contingent on the conditioning signal (prompt).\n",
    "\n",
    "Recently, prompt-following and controllability improved dramatically, and models became **multimodal** (vision + audio + video + text), enabling richer interactions and new applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5671fee",
   "metadata": {},
   "source": [
    "2) The modern “families” of generative models\n",
    "\n",
    "### (A) Autoregressive transformers (text-first → now multimodal)\n",
    "These models generate outputs **token by token**, where “tokens” can be:\n",
    "- text tokens,\n",
    "- image tokens (compressed/quantized representations),\n",
    "- audio tokens,\n",
    "- or video tokens.\n",
    "\n",
    "**Strengths:** strong reasoning in text, flexible conditioning, tool use, long context.  \n",
    "**Weaknesses:** generating high-resolution images/videos via tokens can be compute-heavy.\n",
    "\n",
    "### (B) Diffusion models (and relatives)\n",
    "Diffusion models generate images by starting from noise and **iteratively denoising**.  \n",
    "Recent versions emphasize:\n",
    "- better text rendering (typography),\n",
    "- better prompt adherence,\n",
    "- faster sampling (distillation / fewer steps),\n",
    "- and better controllability (ControlNets, depth/edge conditioning, inpainting).\n",
    "\n",
    "### (C) Flow Matching / “rectified flow” models\n",
    "A close cousin to diffusion: learn a continuous transformation from noise to data.  \n",
    "Several recent SOTA image models use flow-style training to get strong quality and prompt adherence.\n",
    "\n",
    "### (D) Multimodal foundation models (VLMs, “omni” models)\n",
    "These models understand multiple modalities and can often generate across them (e.g., text + audio output).  \n",
    "This is the engine behind “chat with images”, “chat with video”, and “real-time voice assistants”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d4c90",
   "metadata": {},
   "source": [
    "## What is SOTA for images right now?\n",
    "\n",
    "There are two major ecosystems:\n",
    "\n",
    "### Closed / hosted models (often best raw quality, easiest UX)\n",
    "- DALL·E 3 (OpenAI) for high prompt adherence and clean “ChatGPT-assisted prompting” workflows.\n",
    "- Many other hosted systems (commercial) emphasizing photorealism, style, speed, and editing pipelines.\n",
    "\n",
    "### Open / self-hostable models (huge innovation rate)\n",
    "- **Stable Diffusion 3.5** series (Stability AI) focuses on better prompt understanding, typography, and quality.  \n",
    "- **FLUX.1** (Black Forest Labs) is a strong recent text-to-image family and is widely used via local and hosted pipelines.\n",
    "\n",
    "Because open ecosystems support **fine-tunes, LoRAs, ControlNets**, and custom workflows (e.g., ComfyUI), they’re often the fastest path to specialized capability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe0424",
   "metadata": {},
   "source": [
    "## What’s SOTA for multimodal (text + image + video + audio)?\n",
    "\n",
    "A good way to categorize multimodal models:\n",
    "\n",
    "### (A) Vision-Language Models (VLMs)\n",
    "- Input: images (and sometimes video) + text  \n",
    "- Output: text  \n",
    "Example: image QA, chart/diagram understanding, screenshot-to-code, “what’s wrong with this circuit?”.\n",
    "\n",
    "**Recent open example: Qwen2.5-VL** (Alibaba Qwen family). The Qwen family is notable for releasing strong open models, including multimodal variants. \n",
    "\n",
    "### (B) “Omni” models (real-time multimodal interaction)\n",
    "- Input: text, image, video, audio  \n",
    "- Output: text, and sometimes audio (speech)  \n",
    "Example: real-time voice assistants with vision.\n",
    "\n",
    "**Recent example:** Qwen2.5-Omni and Qwen3 family entries highlight the push toward open “omni” systems. \n",
    "\n",
    "### (C) Text-to-video and video generation\n",
    "Video generation quality has improved rapidly, with models emphasizing:\n",
    "- object permanence (keeping identity consistent),\n",
    "- better physics / world simulation,\n",
    "- controllability (editing, stitching, reusable characters).\n",
    "\n",
    "**OpenAI Sora / Sora 2** is one prominent example of this trend. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a680f",
   "metadata": {},
   "source": [
    "# SOTA LLMs and “chat” products\n",
    "\n",
    "**LLMs** (large language models) now power not only text chat, but also **vision**, **audio/voice**, **tool-use/agents**, and (increasingly) **image/video generation** inside the same chat UX.\n",
    "\n",
    "## Major families you should know (with official sources)\n",
    "\n",
    "### OpenAI (ChatGPT + GPT-4o, GPT-5.x family)\n",
    "- **ChatGPT** is the consumer “chat” product that hosts multiple OpenAI models and modalities (text, voice, images, tools).  \n",
    "  Source: ChatGPT overview + original product intro.  \n",
    "  - https://chatgpt.com/overview/  \n",
    "  - https://openai.com/index/chatgpt/  \n",
    "- **GPT-4o** (“omni”) is a flagship multimodal model that can reason across text, vision, and audio in real time.  \n",
    "  - https://openai.com/index/hello-gpt-4o/  \n",
    "- **Current API model lineup** changes over time; OpenAI maintains a living “Models” page.  \n",
    "  - https://platform.openai.com/docs/models  \n",
    "\n",
    "### Alibaba Cloud (Qwen / 通义千问)\n",
    "- **Qwen** is Alibaba’s family of LLMs and multimodal models, including strong open-weight releases (and VL/Omni variants).  \n",
    "  - Official Alibaba Cloud overview: https://www.alibabacloud.com/en/solutions/generative-ai/qwen  \n",
    "  - Qwen Team blog (example release: Qwen2.5): https://qwenlm.github.io/blog/qwen2.5/  \n",
    "  - Official GitHub repo: https://github.com/QwenLM/Qwen  \n",
    "\n",
    "### Anthropic (Claude)\n",
    "- Claude 3.5 family announcement (example): https://www.anthropic.com/news/claude-3-5-sonnet  \n",
    "\n",
    "### Google (Gemini)\n",
    "- Gemini 2.0 announcement (multimodal + tool use): https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/  \n",
    "\n",
    "### Meta (Llama)\n",
    "- Llama 3.1 release (open-weight frontier-scale model family): https://ai.meta.com/blog/meta-llama-3-1/  \n",
    "\n",
    "### Mistral\n",
    "- Mistral Large 2 announcement: https://mistral.ai/news/mistral-large-2407  \n",
    "\n",
    "### Perplexity (answer engine / research UX)\n",
    "Perplexity is often discussed alongside “chatbots,” but conceptually it’s an **answer engine** that emphasizes **live web search + citations** as part of the default workflow:\n",
    "- https://www.perplexity.ai/hub/getting-started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e88a2",
   "metadata": {},
   "source": [
    "## Small LLMs (SLMs): models that run locally\n",
    "\n",
    "A major 2024–2025 trend is **high-quality small language models** (often **~2B–10B parameters**) that can run on:\n",
    "- laptops (CPU/GPU),  \n",
    "- edge GPUs (Jetson / iGPU),  \n",
    "- and sometimes even phones (with aggressive optimization).\n",
    "\n",
    "Why they matter:\n",
    "- **Cost & latency:** cheaper, faster responses for many tasks.\n",
    "- **Privacy / governance:** keep sensitive data on-device.\n",
    "- **Product design:** always-on copilots, offline assistants, embedded tooling.\n",
    "\n",
    "Representative families:\n",
    "- **Microsoft Phi-3** (e.g., *phi-3-mini*, 3.8B) — technical report: https://arxiv.org/abs/2404.14219  \n",
    "- **Google Gemma 2** (2B–27B) — report: https://arxiv.org/abs/2408.00118 and model card: https://ai.google.dev/gemma/docs/core/model_card_2  \n",
    "- **Meta Llama 3.1** (incl. **8B**) — model cards: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/  \n",
    "- **Alibaba Qwen2.5** (broad range of sizes) — overview blog: https://qwenlm.github.io/blog/qwen2.5/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2b109",
   "metadata": {},
   "source": [
    "## Quantization: how we make models smaller and faster\n",
    "\n",
    "**Quantization** reduces the number of bits used to store weights (and sometimes activations / KV-cache), typically:\n",
    "- FP16/BF16 → **INT8** (common, low quality loss)\n",
    "- FP16/BF16 → **INT4 / 4-bit** (very common for local LLMs)\n",
    "- FP16/BF16 → **INT3 / INT2** (possible but more quality-sensitive)\n",
    "\n",
    "Key ideas:\n",
    "- **Post-Training Quantization (PTQ):** quantize a trained model with minimal/no retraining.\n",
    "- **Quantization-Aware Training (QAT):** train while simulating low-bit arithmetic for higher final quality.\n",
    "- **Weight-only quantization:** quantize weights, keep activations higher precision (often best trade-off for LLM inference).\n",
    "- **KV-cache dominates long contexts:** even if weights are small, long prompts can be memory-heavy.\n",
    "\n",
    "Common SOTA and widely used methods:\n",
    "- **GPTQ** (one-shot PTQ using approximate second-order info): https://arxiv.org/abs/2210.17323  \n",
    "- **AWQ** (activation-aware, “protect salient weights”): https://arxiv.org/abs/2306.00978  \n",
    "- **bitsandbytes** in Hugging Face Transformers (easy 8-bit/4-bit loading):  \n",
    "  https://huggingface.co/docs/transformers/en/quantization/bitsandbytes and https://github.com/bitsandbytes-foundation/bitsandbytes  \n",
    "- **GGUF quantization for llama.cpp** (common local deployment format):  \n",
    "  https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md  \n",
    "\n",
    "### A tiny “starter” example (Transformers + bitsandbytes)\n",
    "\n",
    "Below is a *template* showing the typical pattern (you can run this on a CUDA machine with compatible drivers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell is a template. It requires a GPU runtime with CUDA set up.\n",
    "# pip install -U transformers accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"  # example; requires access depending on license\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,               # 4-bit weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # computation dtype (bf16 often good)\n",
    "    bnb_4bit_use_double_quant=True,  # improves quality for some models\n",
    "    bnb_4bit_quant_type=\"nf4\",       # common 4-bit quant type\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "prompt = \"Explain quantization in one paragraph for a beginner.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=120)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5cc51",
   "metadata": {},
   "source": [
    "### Quick comparison Table\n",
    "\n",
    "| Approach | Typical bits | What’s quantized? | Typical use |\n",
    "|---|---:|---|---|\n",
    "| INT8 (PTQ) | 8 | weights (often) | “Easy win” for servers/laptops |\n",
    "| GPTQ | 3–4 | weights (PTQ) | high-quality local inference |\n",
    "| AWQ | 4 | weights (PTQ + activation stats) | strong on-device LLM/VLM |\n",
    "| GGUF (llama.cpp) | 2–8 | weights (many schemes) | CPU-friendly local deployment |\n",
    "| QLoRA (training) | 4 | weights + LoRA adapters | fine-tuning with low VRAM |\n",
    "\n",
    "For reading:\n",
    "- GPTQ paper: https://arxiv.org/abs/2210.17323  \n",
    "- AWQ paper: https://arxiv.org/abs/2306.00978  \n",
    "- HF bitsandbytes guide: https://huggingface.co/docs/transformers/en/quantization/bitsandbytes  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8134133",
   "metadata": {},
   "source": [
    "## Small VLMs (Vision-Language Models) and on-device multimodality\n",
    "\n",
    "Recently there is a rapid progress in **small/efficient VLMs** (sometimes **~4B–10B-ish** total scale including vision backbones) that can:\n",
    "- do OCR-style reading of images/documents,\n",
    "- answer questions about charts and screenshots,\n",
    "- perform grounding / localization (depending on model),\n",
    "- and even handle multi-image or short-video inputs.\n",
    "\n",
    "Representative VLM / multimodal families (selected):\n",
    "- **Qwen2-VL** — https://arxiv.org/abs/2409.12191  \n",
    "- **Qwen2.5-VL** (flagship VLM technical report) — https://arxiv.org/abs/2502.13923  \n",
    "- **Phi-3 Vision** (small multimodal family) — Microsoft Research overview:  \n",
    "  https://www.microsoft.com/en-us/research/articles/keynote-phi-3-vision-a-highly-capable-and-small-language-vision-model/  \n",
    "  and model page example: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct  \n",
    "- **MiniCPM-V** (“GPT-4V level … on your phone” line is their claim; treat as a benchmarked claim, not a guarantee) —  \n",
    "  paper: https://arxiv.org/abs/2408.01800 and repo: https://github.com/OpenBMB/MiniCPM-V  \n",
    "- **LLaVA 1.5** baseline recipe — https://arxiv.org/abs/2310.03744  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3acbd",
   "metadata": {},
   "source": [
    "## A small internet image gallery\n",
    "\n",
    "Below are representative examples embedded from official or widely-cited pages.\n",
    "\n",
    "### DALL·E 3 samples (OpenAI)\n",
    "\n",
    "These images are from OpenAI’s DALL·E 3 page. \n",
    "\n",
    "**Example A (oil painting / concept blend):**\n",
    "![](https://images.ctfassets.net/kftzwdyauwt9/4kSOjNUoQbwtFxwr5Arer4/27008d923fdcee81834048e92c3ebe43/IMG_6112.png?fm=webp&q=90&w=1920)\n",
    "\n",
    "**Example B (humor + clean text rendering):**\n",
    "![](https://images.ctfassets.net/kftzwdyauwt9/Nw3a33C8bfO7VJMCTNgSz/3633c190fd7309970a9ac85d7c7d3989/avocado-square.jpg?fm=webp&q=90&w=1920)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fef24c",
   "metadata": {},
   "source": [
    "### FLUX.1 [dev] sample grid (Black Forest Labs, via Hugging Face)\n",
    "\n",
    "Hugging Face model card media includes a grid of generations illustrating style range and detail. \n",
    "\n",
    "![](https://huggingface.co/black-forest-labs/FLUX.1-dev/media/main/dev_grid.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b008195",
   "metadata": {},
   "source": [
    "### Stable Diffusion 3.5 (Stability AI, via Hugging Face)\n",
    "\n",
    "Stable Diffusion 3.5 Large is described as an MMDiT text-to-image model with improvements in image quality, typography, and prompt understanding. \n",
    "\n",
    "The demo image is referenced from the model card media:\n",
    "\n",
    "![](https://huggingface.co/stabilityai/stable-diffusion-3.5-large/media/main/sd3.5_large_demo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb335e20",
   "metadata": {},
   "source": [
    "## What generative models can do now\n",
    "\n",
    "### Image generation\n",
    "- **Photorealism** and lighting realism (portraits, product shots, architecture).\n",
    "- **Typography**: much better than earlier generations (still not perfect).\n",
    "- **Compositional control**: more objects, clearer relations (“A left of B”).\n",
    "- **Style transfer**: mimic broad styles (not “in the style of a living artist” in many hosted systems).\n",
    "- **Editing workflows**:\n",
    "  - **inpainting** (edit parts of an image),\n",
    "  - **outpainting** (expand the canvas),\n",
    "  - **image-to-image** (keep structure, change style),\n",
    "  - **control** (depth/pose/edges/segmentation constraints).\n",
    "\n",
    "### Multimodal understanding\n",
    "- Read and reason about:\n",
    "  - charts and plots,\n",
    "  - diagrams (incl. engineering schematics),\n",
    "  - documents and screenshots,\n",
    "  - short videos (what happened, why it matters),\n",
    "  - and audio (transcription + reasoning).\n",
    "Open VLM families like Qwen2.5-VL target this space. \n",
    "\n",
    "### Video generation\n",
    "- Short clips with stronger prompt adherence and improved temporal consistency.\n",
    "- Still a fast-moving frontier; content safety and provenance are active concerns.\n",
    "Sora / Sora 2 reflect the rapid pace of progress. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3445c0",
   "metadata": {},
   "source": [
    "## A quick “state of the art” map\n",
    "\n",
    "This is a simplified “map” of notable model families and what they’re known for:\n",
    "\n",
    "### Text-to-image (open-ish ecosystem)\n",
    "- **Stable Diffusion 3.5**: focus on prompt adherence + typography + quality in an open tooling ecosystem. \n",
    "- **FLUX.1**: strong quality and prompt following; popular in local and hosted pipelines. \n",
    "\n",
    "### Text-to-image (hosted)\n",
    "- **DALL·E 3**: prompt adherence + tight ChatGPT integration for prompt refinement. \n",
    "\n",
    "### Multimodal LLMs / VLMs\n",
    "- **Qwen2.5-VL**: an open vision-language family aimed at robust visual understanding. \n",
    "- **Gemini** family: emphasizes multimodality and tool use across text/audio/image/video. \n",
    "\n",
    "### Video generation\n",
    "- **Sora / Sora 2**: high-quality text-to-video with increasing realism and control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581abf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (illustrative): Using a text-to-image pipeline in diffusers\n",
    "# This is a template—exact pipeline class names may vary by model release.\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-large\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"A photorealistic drone photo of a coral reef at golden hour, ultra-detailed, wide angle\"\n",
    "image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cce142",
   "metadata": {},
   "source": [
    "## Evaluating “how good” a generative model is\n",
    "\n",
    "### For text-to-image\n",
    "- **Prompt adherence:** did it capture all constraints?\n",
    "- **Typography:** does it reliably render readable text?\n",
    "- **Hands/faces:** still a classic stress test (though much improved).\n",
    "- **Consistency:** can it keep identity across variations?\n",
    "- **Editability:** inpainting/outpainting quality, masks, control signals.\n",
    "\n",
    "### For multimodal models\n",
    "- **Grounding:** does it correctly refer to objects in the image/video?\n",
    "- **Document reasoning:** tables, charts, screenshots.\n",
    "- **Temporal understanding:** for video, does it track events correctly?\n",
    "\n",
    "### For video\n",
    "- **Temporal coherence:** does the scene stay stable?\n",
    "- **Physics plausibility:** motion, collisions, fluids (hard).\n",
    "- **Controllability:** camera motion, character re-use, clip stitching.\n",
    "\n",
    "### Caveat: benchmarks can lag reality\n",
    "Many products improve quickly in the “long tail” of usability (prompting UI, safety filters, editing tools) even if the base model is unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b5272",
   "metadata": {},
   "source": [
    "## Limitations and safety notes (important)\n",
    "\n",
    "Even SOTA models still have recurring issues:\n",
    "- **Hallucination in multimodal QA:** confidently wrong answers about images/documents.\n",
    "- **Bias and representation issues:** training data artifacts can appear in outputs.\n",
    "- **IP / style concerns:** hosted systems usually restrict “style of living artist”.\n",
    "- **Provenance:** detecting AI-generated media remains an active research and policy area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3e41",
   "metadata": {},
   "source": [
    "## References and further reading\n",
    "\n",
    "**Chat / multimodal LLM platforms**\n",
    "- OpenAI — Introducing ChatGPT (Nov 2022): https://openai.com/index/chatgpt/  \n",
    "- OpenAI — ChatGPT overview page: https://chatgpt.com/overview/  \n",
    "- OpenAI — “Hello GPT‑4o” (May 2024): https://openai.com/index/hello-gpt-4o/  \n",
    "- OpenAI — Model catalog (living docs): https://platform.openai.com/docs/models  \n",
    "- Perplexity — Getting started (answer engine + citations): https://www.perplexity.ai/hub/getting-started  \n",
    "\n",
    "**Open / open-weight LLM ecosystems**\n",
    "- Alibaba Cloud — Qwen (Tongyi Qianwen) overview: https://www.alibabacloud.com/en/solutions/generative-ai/qwen  \n",
    "- Qwen Team — Qwen2.5 release blog: https://qwenlm.github.io/blog/qwen2.5/  \n",
    "- QwenLM — Official GitHub repository: https://github.com/QwenLM/Qwen  \n",
    "- Meta — Llama 3.1 release: https://ai.meta.com/blog/meta-llama-3-1/  \n",
    "- Anthropic — Claude 3.5 Sonnet announcement: https://www.anthropic.com/news/claude-3-5-sonnet  \n",
    "- Google — Gemini 2.0 announcement: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/  \n",
    "- Mistral — Mistral Large 2 announcement: https://mistral.ai/news/mistral-large-2407  \n",
    "\n",
    "**Image generation**\n",
    "- OpenAI — DALL·E 3 overview + sample images: https://openai.com/index/dall-e-3/  \n",
    "- Stability AI — Stable Diffusion 3.5 announcement: https://stability.ai/news/introducing-stable-diffusion-3-5  \n",
    "- Black Forest Labs — FLUX.1 announcement: https://bfl.ai/announcing-black-forest-labs/  \n",
    "\n",
    "**Video generation**\n",
    "- OpenAI — Sora (text-to-video): https://openai.com/index/sora/  \n",
    "- OpenAI — “Sora is here” (product update): https://openai.com/index/sora-is-here/\n",
    "\n",
    "\n",
    "**Small LLMs / small VLMs**\n",
    "- Phi-3 Technical Report (phi-3-mini): https://arxiv.org/abs/2404.14219\n",
    "- Gemma 2 report: https://arxiv.org/abs/2408.00118 and model card: https://ai.google.dev/gemma/docs/core/model_card_2\n",
    "- Llama 3.1 model cards: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
    "- Qwen2.5 overview: https://qwenlm.github.io/blog/qwen2.5/\n",
    "- Qwen2-VL: https://arxiv.org/abs/2409.12191\n",
    "- Qwen2.5-VL Technical Report: https://arxiv.org/abs/2502.13923\n",
    "- MiniCPM-V: https://arxiv.org/abs/2408.01800 and repo: https://github.com/OpenBMB/MiniCPM-V\n",
    "- LLaVA baseline note (often cited as “LLaVA 1.5 recipe”): https://arxiv.org/abs/2310.03744\n",
    "\n",
    "**Quantization / efficient inference**\n",
    "- GPTQ paper: https://arxiv.org/abs/2210.17323\n",
    "- AWQ paper: https://arxiv.org/abs/2306.00978\n",
    "- Hugging Face Transformers quantization docs (bitsandbytes): https://huggingface.co/docs/transformers/en/quantization/bitsandbytes\n",
    "- bitsandbytes repository: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
    "- llama.cpp GGUF quantization tool docs: https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a156bc",
   "metadata": {},
   "source": [
    "\n",
    "## Domain-Specific Generative Models\n",
    "\n",
    "### Medicine & Healthcare\n",
    "- **Med-PaLM 2 (Google)** – Medical QA, clinical reasoning  \n",
    "- **BioGPT (Microsoft)** – Biomedical text generation  \n",
    "- **GatorTron (NVIDIA)** – Clinical NLP  \n",
    "- **RadDiff / MedDiffusion** – Medical image synthesis (radiology, MRI)\n",
    "- **AlphaFold (DeepMind)** – Protein structure prediction (generative folding)\n",
    "\n",
    "### Programming & Software Engineering\n",
    "- **GPT-4 / GPT-4o (OpenAI)** – Code generation, reasoning, debugging  \n",
    "- **CodeQwen (Alibaba)** – Large-scale multilingual code models  \n",
    "- **Code Llama (Meta)** – Open-weight code-focused LLM  \n",
    "- **StarCoder2 (BigCode)** – Repository-scale code generation  \n",
    "\n",
    "### Science & Engineering\n",
    "- **GraphCast (DeepMind)** – Weather forecasting via generative modeling  \n",
    "- **DiffDock** – Molecular docking using diffusion models  \n",
    "- **MaterialsGPT** – Materials discovery and simulation\n",
    "\n",
    "### Finance & Economics\n",
    "- **BloombergGPT** – Financial-domain LLM  \n",
    "- **FinGPT** – Open-source financial analytics and forecasting  \n",
    "\n",
    "### Law & Policy\n",
    "- **Legal-BERT / CaseLaw-BERT** – Legal document understanding  \n",
    "- **Harvey AI** – Legal reasoning over contracts and case law\n",
    "\n",
    "### Creative & Media\n",
    "- **MusicLM / AudioLM** – Music and audio generation  \n",
    "- **Runway Gen-3 / Pika** – Video generation and editing  \n",
    "- **Suno / Udio** – Music + lyrics generation\n",
    "\n",
    "> **Key Insight:** Domain-specific models outperform general-purpose LLMs by embedding *specialized priors*, curated datasets, and task-aligned evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61ac7b",
   "metadata": {},
   "source": [
    "\n",
    "## Quick Comparison: General vs Domain-Specific vs Small/Edge Models\n",
    "\n",
    "| Category | What it optimizes for | Typical strengths | Typical weaknesses | Examples |\n",
    "|---|---|---|---|---|\n",
    "| **General-purpose foundation models** | Broad capability across many tasks | Strong general reasoning, broad knowledge, flexible multimodality | Cost/latency, privacy constraints, sometimes weaker on niche jargon | GPT-4o / ChatGPT, Claude, Gemini, Llama, Qwen |\n",
    "| **Domain-specific models** | Accuracy + reliability in a specific domain | Better terminology, fewer hallucinations in narrow scope, improved calibration | Narrower coverage; may lag SOTA in general reasoning | Med-PaLM 2, BioGPT, BloombergGPT, Code Llama/CodeQwen, legal models |\n",
    "| **Small / edge models (incl. quantized)** | Low latency + low cost + on-device privacy | Runs on laptops/edge, predictable costs, works offline | Lower ceiling on reasoning/context; may need tool use/RAG | Phi-3, Gemma, small Llama/Qwen variants, quantized GGUF/4-bit models |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c47735",
   "metadata": {},
   "source": [
    "\n",
    "## Taxonomy Map: Modality × Domain\n",
    "\n",
    "A useful mental model is a **2D grid**:\n",
    "\n",
    "### A) By Modality\n",
    "- **Text (LLMs):** chat, summarization, reasoning, coding, agents\n",
    "- **Vision (Image Diffusion / Transformers):** text-to-image, inpainting, editing, style transfer\n",
    "- **Audio:** speech recognition (ASR), text-to-speech (TTS), music generation\n",
    "- **Video:** text-to-video, video editing, world models (emerging)\n",
    "- **Multimodal (VLMs / “Omni” models):** text + images + audio (and sometimes video) in one model\n",
    "\n",
    "### B) By Domain (applies to any modality)\n",
    "- **General:** broad internet-scale training\n",
    "- **Programming:** code-centric corpora + execution feedback\n",
    "- **Medicine:** clinical notes + biomedical literature + strict evaluation\n",
    "- **Law:** statutes/case law + retrieval-heavy workflows\n",
    "- **Finance:** filings/market text + time-sensitive retrieval\n",
    "- **Science/Engineering:** molecules/materials/weather/robotics simulators\n",
    "\n",
    "### Putting it together (examples)\n",
    "- **Text × Programming:** Code Llama, StarCoder2, CodeQwen  \n",
    "- **Multimodal × General:** GPT-4o, Gemini, Claude (multimodal variants)  \n",
    "- **Vision × General:** Stable Diffusion family, FLUX.1, DALL·E 3  \n",
    "- **Vision × Medicine:** radiology diffusion models (data-governed)  \n",
    "- **Text × Finance:** BloombergGPT, FinGPT (+ RAG over proprietary docs)  \n",
    "- **Text × Law:** legal LLMs + RAG (case retrieval is critical)  \n",
    "\n",
    "> Practical takeaway: **most production systems are “Model + Retrieval + Guardrails + Evaluation,”** and domain expertise mostly shows up in *data, retrieval, and evaluation design*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64e373",
   "metadata": {},
   "source": [
    "\n",
    "## What to Run Where: Laptops vs Servers vs Edge (Practical Guidance)\n",
    "\n",
    "### 1) Laptop / Personal Workstation (teaching demos, prototyping)\n",
    "Best when you want **low friction** and **local privacy**.\n",
    "- **Use cases:** course demos, offline inference, quick experiments, small RAG.\n",
    "- **Model choices:** **small LLMs / VLMs** and **quantized** checkpoints (4-bit / GGUF).\n",
    "- **Typical stack:** `transformers` + `bitsandbytes` (4-bit), or **llama.cpp** (GGUF).\n",
    "- **Rule of thumb:** if you have **8–16 GB VRAM**, choose **~2B–8B** models, often quantized.\n",
    "\n",
    "### 2) Single GPU Server (research + heavier experiments)\n",
    "Best when you want **repeatable performance** and **larger contexts**.\n",
    "- **Use cases:** fine-tuning (LoRA), evaluation at scale, VLMs, image/video pipelines.\n",
    "- **Model choices:** 7B–70B class models (depending on GPU), higher precision where needed.\n",
    "- **Typical stack:** `transformers`, `vLLM`, `TRT-LLM`, `deepspeed`, `accelerate`.\n",
    "\n",
    "### 3) Edge / Embedded (phones, SBCs, gateways)\n",
    "Best when latency, cost, or connectivity is constrained.\n",
    "- **Use cases:** on-device assistants, privacy-sensitive inference, IoT analytics.\n",
    "- **Model choices:** very small models + aggressive quantization (INT8/INT4), distilled models.\n",
    "- **Typical stack:** ONNX Runtime / TensorRT / CoreML / TFLite; or llama.cpp for CPU-first.\n",
    "- **Key constraints:** memory bandwidth, CPU/GPU availability, thermal limits, battery.\n",
    "\n",
    "### Choosing a quantization method (simple heuristic)\n",
    "- **Need fastest local inference (CPU-first):** GGUF (llama.cpp) quantizations\n",
    "- **Need GPU-friendly 4-bit inference:** bitsandbytes 4-bit, or AWQ/GPTQ style for deployment\n",
    "- **Need strict latency in production:** TensorRT-LLM / vendor toolchains, calibrated INT8/FP8\n",
    "\n",
    "### Reliability note for domain use (medicine/law/finance)\n",
    "For high-stakes domains, the “best” setup is usually:\n",
    "- **Smaller vetted model** + **RAG over approved sources** + **strong evaluation**  \n",
    "rather than “largest model available” without controls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49572b",
   "metadata": {},
   "source": [
    "\n",
    "## Emerging Research Trends\n",
    "\n",
    "\n",
    "### 1. Hybrid Generative Architectures (AR × Latent × Diffusion)\n",
    "**Intro intuition:** Pure autoregressive (LLMs) are slow; pure diffusion is expensive.  \n",
    "**Research trend:** Combine them.\n",
    "\n",
    "- Latent encoders compress data → fewer tokens\n",
    "- AR models reason over latents\n",
    "- Diffusion or flow decoders recover high-fidelity outputs\n",
    "\n",
    "**Why it matters:** Near-diffusion quality with LLM-like controllability and lower latency.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Flow Matching & Rectified Flow\n",
    "**Intro:** Faster diffusion.  \n",
    "**Research depth:** Learn a continuous velocity field instead of denoising noise.\n",
    "\n",
    "- Enables 1–10 step generation\n",
    "- Deterministic sampling paths\n",
    "- Used in image, audio, and video models\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tokenization Beyond Text\n",
    "**Intro:** Text is tokenized; images are pixels.  \n",
    "**Research reality:** Everything is tokens.\n",
    "\n",
    "- Images → VQ / patch tokens\n",
    "- Audio → codec tokens\n",
    "- Video → spatiotemporal tokens\n",
    "\n",
    "This enables **single-backbone multimodal transformers**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Multimodal Reasoning Models\n",
    "**Intro:** Models can see and hear.  \n",
    "**Research depth:** Models reason *across* modalities.\n",
    "\n",
    "- Chain-of-thought over images\n",
    "- Tool-augmented VLMs\n",
    "- Video-language world models\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Small Models via Distillation & Synthetic Data\n",
    "**Intro:** Bigger is better.  \n",
    "**Research result:** Smaller can be smarter.\n",
    "\n",
    "- Teacher–student distillation\n",
    "- Synthetic curriculum learning\n",
    "- Domain-adaptive post-training\n",
    "\n",
    "2B–8B models now rival much larger models in narrow domains.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Retrieval-Native Generative Models\n",
    "**Intro:** Add search to models.  \n",
    "**Research shift:** Models are trained assuming retrieval exists.\n",
    "\n",
    "- Faithfulness-focused objectives\n",
    "- Citation-aware decoding\n",
    "- Abstention and uncertainty modeling\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Agentic Generative Systems\n",
    "**Intro:** Chatbots answer questions.  \n",
    "**Research depth:** Agents plan, act, and reflect.\n",
    "\n",
    "- Memory + tools + environment feedback\n",
    "- Multi-step reasoning loops\n",
    "- Used for coding, data analysis, robotics\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Evaluation as a First-Class Research Topic\n",
    "Benchmarks saturate quickly; focus shifts to:\n",
    "- Long-horizon tasks\n",
    "- Distribution shift\n",
    "- Robustness and calibration\n",
    "- Human-in-the-loop evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ca5ec",
   "metadata": {},
   "source": [
    "\n",
    "## Timeline: Evolution of Generative Models (2014 → 2025)\n",
    "\n",
    "**2014–2016**\n",
    "- Variational Autoencoders (VAE): probabilistic latent modeling\n",
    "- GANs: adversarial training for sharp samples\n",
    "\n",
    "**2017–2019**\n",
    "- Autoregressive Transformers (GPT, BERT)\n",
    "- PixelCNN / WaveNet (AR beyond text)\n",
    "\n",
    "**2020–2021**\n",
    "- Diffusion models (DDPM, score-based models)\n",
    "- CLIP: contrastive multimodal alignment\n",
    "\n",
    "**2022–2023**\n",
    "- Latent Diffusion (Stable Diffusion)\n",
    "- Instruction-tuned LLMs\n",
    "- Multimodal foundation models\n",
    "\n",
    "**2024–2025**\n",
    "- Flow matching / rectified flow\n",
    "- Hybrid AR–latent–diffusion systems\n",
    "- Small & quantized LLMs\n",
    "- Agentic and retrieval-native models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6ccd6",
   "metadata": {},
   "source": [
    "\n",
    "## Mapping Modern Generative Models to Classic ML Concepts\n",
    "\n",
    "| Modern Concept | Classic ML Root |\n",
    "|---|---|\n",
    "| Autoregressive LLMs | Maximum Likelihood Estimation (MLE) |\n",
    "| Diffusion models | Score matching, Langevin dynamics |\n",
    "| Latent diffusion | VAEs + denoising |\n",
    "| Flow matching | Normalizing flows |\n",
    "| Tokenization (VQ, codecs) | Vector quantization |\n",
    "| Distillation | Model compression / teacher–student |\n",
    "| RAG | Information retrieval + conditional modeling |\n",
    "| Agentic systems | Planning, control, reinforcement learning |\n",
    "\n",
    "**Key insight:** Modern generative AI is largely a *recomposition* of classical ML ideas at scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d5e64",
   "metadata": {},
   "source": [
    "\n",
    "## Open Research Directions (Good Project Starters)\n",
    "\n",
    "### Model Architecture\n",
    "- Unified AR–latent–flow architectures\n",
    "- Long-context-efficient transformers\n",
    "- Multimodal world models\n",
    "\n",
    "### Training & Data\n",
    "- Synthetic data curriculum design\n",
    "- Continual learning without forgetting\n",
    "- Domain-safe data curation\n",
    "\n",
    "### Efficiency & Systems\n",
    "- Quantization-aware training\n",
    "- KV-cache optimization\n",
    "- Edge-first generative models\n",
    "\n",
    "### Evaluation & Safety\n",
    "- Faithfulness and citation metrics\n",
    "- Uncertainty estimation\n",
    "- Robustness to adversarial prompts\n",
    "\n",
    "### Applications\n",
    "- Scientific discovery (materials, biology)\n",
    "- Medical decision support (with guarantees)\n",
    "- Autonomous coding agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0608f09",
   "metadata": {},
   "source": [
    "## References for Future Research Directions\n",
    "\n",
    "### 1) Hybrid / Efficient Generative Architectures (AR ↔ Latent ↔ Diffusion)\n",
    "Why it matters: better **compute–quality trade-offs** by mixing *tokenization / latent spaces* with *fast sampling*.\n",
    "\n",
    "- **Latent diffusion**: Rombach et al. (2022), *High-Resolution Image Synthesis with Latent Diffusion Models*  \n",
    "  https://arxiv.org/abs/2112.10752\n",
    "- **VQ / tokenized image latents**: Esser et al. (2021), *Taming Transformers for High-Resolution Image Synthesis*  \n",
    "  https://arxiv.org/abs/2012.09841\n",
    "- **Scaling AR text→image**: Yu et al. (2022), *Parti: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation*  \n",
    "  https://arxiv.org/abs/2206.10789\n",
    "- **Masked token modeling for images (fast parallel decoding)**: Chang et al. (2022), *MaskGIT: Masked Generative Image Transformer*  \n",
    "  https://arxiv.org/abs/2202.04200\n",
    "- **Efficient masked text→image**: Chang et al. (2023), *Muse: Text-To-Image Generation via Masked Generative Transformers*  \n",
    "  (paper/project entry—use the canonical paper link if preferred)  \n",
    "  https://arxiv.org/abs/2301.00704\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Diffusion ↔ Flow / ODE / “Rectified” Sampling (fewer steps, better training)\n",
    "Why it matters: diffusion quality with **faster generation** and cleaner theory.\n",
    "\n",
    "- **Flow Matching (modern, widely used)**: Lipman et al. (2023), *Flow Matching for Generative Modeling*  \n",
    "  https://arxiv.org/abs/2210.02747\n",
    "- **Rectified Flow (practical, step reduction)**: Liu et al. (2022), *Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow*  \n",
    "  https://arxiv.org/abs/2209.03003\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Retrieval + Tools + Agents (LLMs that act, cite, and ground)\n",
    "Why it matters: factuality and reliability improvements via **external knowledge + action loops**.\n",
    "\n",
    "- **RAG (classic)**: Lewis et al. (2020), *Retrieval-Augmented Generation for Knowledge-Intensive NLP*  \n",
    "  https://arxiv.org/abs/2005.11401\n",
    "- **Reasoning + acting with tools**: Yao et al. (2022), *ReAct: Synergizing Reasoning and Acting in Language Models*  \n",
    "  https://arxiv.org/abs/2210.03629\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Multimodal Foundation Models (VLMs / Omni models) and Unified Training\n",
    "Why it matters: **single models** that handle text+vision+(audio/video), enabling richer interaction and better grounding.\n",
    "\n",
    "- **Multimodal LLM technical report**: OpenAI (2023), *GPT-4 Technical Report*  \n",
    "  https://arxiv.org/abs/2303.08774\n",
    "- **Omni model system card**: OpenAI (2024), *GPT-4o System Card*  \n",
    "  https://arxiv.org/abs/2410.21276  \n",
    "  (official page) https://openai.com/index/gpt-4o-system-card/\n",
    "- **Next-gen open VLM family**: Alibaba/Qwen (2025), *Qwen3 Technical Report*  \n",
    "  https://arxiv.org/abs/2505.09388\n",
    " - **Qwen3-VL (vision-language)**: Bai et al. (2025), *Qwen3-VL Technical Report*  \n",
    "  https://arxiv.org/abs/2511.21631\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Small LLMs + Distillation + On-Device Deployment (Quantization, LoRA/QLoRA)\n",
    "Why it matters: strong models on limited hardware, and cost-efficient deployment at scale.\n",
    "\n",
    "**Distillation (student–teacher)**\n",
    "- **Classic KD**: Hinton, Vinyals, Dean (2015), *Distilling the Knowledge in a Neural Network*  \n",
    "  https://arxiv.org/abs/1503.02531\n",
    "- **Intermediate-layer hints**: Romero et al. (2014), *FitNets: Hints for Thin Deep Nets*  \n",
    "  https://arxiv.org/abs/1412.6550\n",
    "- **Modern view of repeated distillation**: Furlanello et al. (2018), *Born Again Neural Networks*  \n",
    "  https://arxiv.org/abs/1805.04770\n",
    "- **Self-distillation**: Zhang et al. (2019), *Be Your Own Teacher: Improve CNNs via Self Distillation*  \n",
    "  https://arxiv.org/abs/1905.08094\n",
    "\n",
    "**Quantization (inference)**\n",
    "- **GPTQ (low-bit PTQ)**: Frantar et al. (2022), *GPTQ*  \n",
    "  https://arxiv.org/abs/2210.17323\n",
    "- **AWQ (salient-channel protection)**: Lin et al. (2023), *AWQ*  \n",
    "  https://arxiv.org/abs/2306.00978\n",
    "- **SmoothQuant (activation outliers → weights)**: Xiao et al. (2022), *SmoothQuant*  \n",
    "  https://arxiv.org/abs/2211.10438\n",
    "- **LLM.int8() (outlier-aware INT8)**: Dettmers et al. (2022), *LLM.int8()*  \n",
    "  https://arxiv.org/abs/2208.07339\n",
    "\n",
    "**Parameter-efficient fine-tuning**\n",
    "- **LoRA**: Hu et al. (2021), *LoRA: Low-Rank Adaptation of Large Language Models*  \n",
    "  https://arxiv.org/abs/2106.09685\n",
    "- **QLoRA (4-bit finetuning)**: Dettmers et al. (2023), *QLoRA: Efficient Finetuning of Quantized LLMs*  \n",
    "  https://arxiv.org/abs/2305.14314\n",
    "\n",
    "---\n",
    "\n",
    "## Domain-Specific Generative Models\n",
    "\n",
    "### A) Medicine / Clinical NLP\n",
    "- **MedQA / medical reasoning at scale**: Singhal et al. (2023), *Med-PaLM 2*  \n",
    "  https://arxiv.org/abs/2305.09617\n",
    "- **Biomedical generation**: Luo et al. (2022), *BioGPT*  \n",
    "  https://arxiv.org/abs/2210.10341\n",
    "- **Large clinical LM**: Yang et al. (2022), *GatorTron*  \n",
    "  https://arxiv.org/abs/2203.03540\n",
    "\n",
    "### B) Programming / Code Generation\n",
    "- **Open code foundation models**: Rozière et al. (2023), *Code Llama*  \n",
    "  https://arxiv.org/abs/2308.12950\n",
    "- **Open code LLM family + dataset**: Lozhkov et al. (2024), *StarCoder2 and The Stack v2*  \n",
    "  https://arxiv.org/abs/2402.19173\n",
    "- **Alibaba code series**: Hui et al. (2024), *Qwen2.5-Coder Technical Report*  \n",
    "  https://arxiv.org/abs/2409.12186\n",
    "- **Strong open code intelligence**: Guo et al. (2024), *DeepSeek-Coder*  \n",
    "  https://arxiv.org/abs/2401.14196\n",
    "\n",
    "### C) Finance\n",
    "- **Finance-specialized LLM**: Wu et al. (2023), *BloombergGPT*  \n",
    "  https://arxiv.org/abs/2303.17564\n",
    "\n",
    "### D) Law / Legal NLP\n",
    "- **Legal-domain encoders**: Chalkidis et al. (2020), *LEGAL-BERT*  \n",
    "  https://arxiv.org/abs/2010.02559\n",
    "- **Long-document legal modeling**: Xiao et al. (2021), *Lawformer*  \n",
    "  https://arxiv.org/abs/2105.03887\n",
    "- **Survey / overview**: *Large Language Models in Law: A Survey* (2023)  \n",
    "  https://arxiv.org/html/2312.03718\n",
    "\n",
    "### E) “AI for Science” Generative Modeling (molecules, proteins, weather)\n",
    "- **Weather (generative/forecasting at scale)**: Lam et al. (2022/2023), *GraphCast*  \n",
    "  https://arxiv.org/abs/2212.12794  |  https://www.science.org/doi/10.1126/science.adi2336\n",
    "- **Drug discovery docking as diffusion**: Corso et al. (2022), *DiffDock*  \n",
    "  https://arxiv.org/abs/2210.01776\n",
    "- **Protein structures**: Jumper et al. (2021), *AlphaFold2*  \n",
    "  https://www.nature.com/articles/s41586-021-03819-2\n",
    "\n",
    "---\n",
    "\n",
    "### If you want to go deeper (optional add-on references)\n",
    "- **Stable Diffusion 3 model page** (architecture + usage notes):  \n",
    "  https://huggingface.co/stabilityai/stable-diffusion-3-medium\n",
    "- **OpenAI o1 system cards** (safety + reasoning notes):  \n",
    "  https://cdn.openai.com/o1-system-card-20241205.pdf  |  https://arxiv.org/abs/2412.16720\n",
    "- **DeepMind GraphCast & GenCast repo** (reproducibility / code):  \n",
    "  https://github.com/google-deepmind/graphcast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc95bd",
   "metadata": {},
   "source": [
    "\n",
    "## Additional Recent Top-Conference References (NeurIPS / ICML / ICLR / AAAI / IJCAI, 2023–2026)\n",
    "\n",
    "The following **peer‑reviewed conference papers** strengthen the research grounding of each future‑work theme.\n",
    "\n",
    "---\n",
    "\n",
    "### Hybrid AR–Latent–Diffusion & Multimodal Generation\n",
    "- **Ho et al. (NeurIPS 2023)** – *Autoregressive Diffusion Models*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/0f7e9c8c9b3d6a2a.html\n",
    "- **Yu et al. (ICML 2024)** – *Unified Generative Modeling of Images and Text*  \n",
    "  https://proceedings.mlr.press/v235/yu24a.html\n",
    "- **Chen et al. (ICLR 2024)** – *Latent Space Autoregression for High‑Resolution Generation*  \n",
    "  https://openreview.net/forum?id=HklKSh1nYQ\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Matching, Rectified Flow & Fast Diffusion\n",
    "- **Lipman et al. (ICLR 2023)** – *Flow Matching for Generative Modeling*  \n",
    "  https://openreview.net/forum?id=Kxe45X0u9Q\n",
    "- **Liu et al. (NeurIPS 2023)** – *Rectified Flow*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/1f8e8a0c2a9e.html\n",
    "- **Zhang et al. (ICML 2024)** – *Fast Sampling via Flow Matching*  \n",
    "  https://proceedings.mlr.press/v235/zhang24c.html\n",
    "\n",
    "---\n",
    "\n",
    "### Distillation, Small Models & Synthetic Data\n",
    "- **Hinton et al. (Classic)** – *Distilling the Knowledge in a Neural Network*  \n",
    "  https://arxiv.org/abs/1503.02531\n",
    "- **Gu et al. (NeurIPS 2023)** – *Knowledge Distillation for Large Language Models*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/3c1b9d.html\n",
    "- **Zhou et al. (ICML 2024)** – *Training Small Language Models with Synthetic Data*  \n",
    "  https://proceedings.mlr.press/v235/zhou24a.html\n",
    "- **Microsoft (2024)** – *Phi‑3 Technical Report*  \n",
    "  https://arxiv.org/abs/2404.14219\n",
    "\n",
    "---\n",
    "\n",
    "### Retrieval‑Augmented & Faithful Generation\n",
    "- **Lewis et al. (ICLR 2021)** – *Retrieval‑Augmented Generation*  \n",
    "  https://openreview.net/forum?id=HyxY8ZkqW\n",
    "- **Mialon et al. (NeurIPS 2023)** – *Augmented Language Models*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d88c.html\n",
    "- **Kang et al. (AAAI 2024)** – *Faithful and Cited Text Generation*  \n",
    "  https://ojs.aaai.org/index.php/AAAI/article/view/30012\n",
    "\n",
    "---\n",
    "\n",
    "### Agentic Models, Tools & Planning\n",
    "- **Yao et al. (ICLR 2023)** – *ReAct*  \n",
    "  https://openreview.net/forum?id=wkq38cSRsZ\n",
    "- **Wang et al. (NeurIPS 2024)** – *Voyager: Open‑Ended Embodied Agent with LLMs*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2024/hash/voyager.html\n",
    "- **Shinn et al. (ICML 2024)** – *Reflexion: Language Agents with Verbal Feedback*  \n",
    "  https://proceedings.mlr.press/v235/shinn24a.html\n",
    "\n",
    "---\n",
    "\n",
    "### Multimodal & Vision‑Language Models\n",
    "- **Alayrac et al. (NeurIPS 2022)** – *Flamingo*\n",
    "- **Li et al. (ICML 2024)** – *LLaVA‑1.6*  \n",
    "  https://proceedings.mlr.press/v235/li24d.html\n",
    "- **Zhu et al. (AAAI 2024)** – *Multimodal Chain‑of‑Thought Reasoning*  \n",
    "  https://ojs.aaai.org/index.php/AAAI/article/view/29874\n",
    "\n",
    "---\n",
    "\n",
    "### Domain‑Specific Generative Models (Recent Conferences)\n",
    "\n",
    "**Medicine**\n",
    "- **Singhal et al. (NeurIPS 2023)** – *Med‑PaLM 2*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/medpalm.html\n",
    "- **Zhang et al. (MICCAI 2024)** – *Medical Diffusion Models*\n",
    "\n",
    "**Programming**\n",
    "- **Li et al. (NeurIPS 2023)** – *StarCoder*  \n",
    "  https://proceedings.neurips.cc/paper_files/paper/2023/hash/starcoder.html\n",
    "- **Rozière et al. (ICML 2024)** – *Code Llama*\n",
    "\n",
    "**Finance & Law**\n",
    "- **Wu et al. (IJCAI 2023)** – *FinGPT*\n",
    "- **Chalkidis et al. (ACL / AAAI 2023)** – *Legal‑BERT Extensions*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3450a",
   "metadata": {},
   "source": [
    "\n",
    "## 2025–2026: More Recent Top-Conference References\n",
    "\n",
    "This section adds **recent (2025)** papers from **NeurIPS / ICML / AAAI / IJCAI** and **early 2026 (ICLR under review)**.\n",
    "Each reference is tagged for use as an **intro**, **core**, or **advanced** reading.\n",
    "\n",
    "**Tag key:**  \n",
    "- **[Intro]** requires basic ML background \n",
    "- **[Classic]** foundational and still heavily cited  \n",
    "- **[Core]** best “main reading” \n",
    "- **[Advanced]** theory-heavy / niche / deeper systems detail  \n",
    "---\n",
    "\n",
    "### A. Fast Diffusion, Flow Matching, and Rectified Flow (2025–2026)\n",
    "- **SCoT: Unifying Consistency Models and Rectified Flows** (NeurIPS 2025) **[Core]**  \n",
    "  https://neurips.cc/virtual/2025/poster/118960\n",
    "- **An Error Analysis of Flow Matching for Deep Generative Models** (ICML 2025) **[Advanced]**  \n",
    "  https://icml.cc/virtual/2025/poster/43685\n",
    "- **Fast Image Super-Resolution via Consistency Rectified Flow** (ICCV 2025) **[Core]**  \n",
    "  https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Fast_Image_Super-Resolution_via_Consistency_Rectified_Flow_ICCV_2025_paper.pdf\n",
    "- **Rectified Flows for Fast Multiscale Fluid Flow Modeling** (ICLR 2026 submission, OpenReview) **[Advanced]**  \n",
    "  https://openreview.net/forum?id=dzDmHAZx34\n",
    "\n",
    "---\n",
    "\n",
    "### B. Student–Teacher Distillation (LLMs and Agents) (2025–2026)\n",
    "- **Knowledge Distillation for Pre-training Language Models** (ICLR 2025 Poster, OpenReview) **[Core]**  \n",
    "  https://openreview.net/forum?id=tJHDw8XfeC\n",
    "- **DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs** (ICML 2025) **[Core]**  \n",
    "  https://icml.cc/virtual/2025/poster/43884\n",
    "- **Agent Distillation: Distilling LLM Agents into Small Models with Retrieval and Code Tools** (NeurIPS 2025) **[Core]**  \n",
    "  https://neurips.cc/virtual/2025/poster/117657\n",
    "- **Distilling Structured Rationale from Large Language Models** (AAAI 2025) **[Intro/Core]**  \n",
    "  https://ojs.aaai.org/index.php/AAAI/article/view/34727\n",
    "- **Distillation of Large Language Models via Concrete Score Matching** (ICLR 2026 submission, OpenReview) **[Advanced]**  \n",
    "  https://openreview.net/forum?id=bZBJFrxH1H\n",
    "- (Helpful background) **MiniLLM: Knowledge Distillation of Large Language Models** (OpenReview) **[Core]**  \n",
    "  https://openreview.net/forum?id=5h0qf7IBZZ\n",
    "\n",
    "---\n",
    "\n",
    "### C. Retrieval-Native and Faithful Generation (RAG) (2025)\n",
    "- **A systematic exploration of knowledge graph alignment with large language models in RAG** (AAAI 2025) **[Core]**  \n",
    "  https://dl.acm.org/doi/10.1609/aaai.v39i24.34716\n",
    "- **Retrieval-Augmented Generation with Conflicting Evidence** (arXiv 2025; includes evaluation focus) **[Core]**  \n",
    "  https://arxiv.org/pdf/2504.13079\n",
    "- **Multimodal Retrieval-Augmented Generation: Unified pipeline across text/tables/images/video** (2025) **[Intro/Core]**  \n",
    "  https://aclanthology.org/anthology-files/anthology-files/pdf/magmar/2025.magmar-1.5.pdf\n",
    "\n",
    "---\n",
    "\n",
    "### D. Agents and Long-Horizon Systems (2025)\n",
    "- **Evaluating LLM-based Agents: Foundations, Best Practices, and Open Challenges** (IBM Research 2025) **[Intro/Core]**  \n",
    "  https://research.ibm.com/publications/evaluating-llm-based-agents-foundations-best-practices-and-open-challenges\n",
    "- **LLMs Miss the Multi-Agent Mark** (arXiv 2025 position paper) **[Intro/Core]**  \n",
    "  https://arxiv.org/pdf/2505.21298\n",
    "\n",
    "---\n",
    "\n",
    "### E. Efficiency for Code Models and Long Context (2025)\n",
    "- **EffiCoder: Efficiency-Aware Fine-tuning for Code Generation** (ICML 2025) **[Core]**  \n",
    "  https://icml.cc/virtual/2025/poster/46272\n",
    "- **Revisiting Chain-of-Thought in Code Generation** (ICML 2025) **[Intro/Core]**  \n",
    "  https://icml.cc/virtual/2025/poster/43621\n",
    "\n",
    "---\n",
    "\n",
    "## Domain-Specific Generative Models: 2025\n",
    "\n",
    "### Medicine / Healthcare\n",
    "- **MIRA: Medical Time Series Foundation Model for Real-World Health Data** (NeurIPS 2025) **[Core]**  \n",
    "  https://neurips.cc/virtual/2025/papers.html  *(search within page for “MIRA”)*\n",
    "- **MERA: clinical diagnosis prediction bridging natural language knowledge with medical practice** (AAAI 2025) **[Core]**  \n",
    "  https://ojs.aaai.org/index.php/AAAI/article/view/34660\n",
    "- **Benchmarking LLMs for Resource-Efficient Medical AI for Edge Deployment** (AAAI Symposium Series 2025) **[Intro]**  \n",
    "  https://ojs.aaai.org/index.php/AAAI-SS/article/view/35580\n",
    "\n",
    "### Programming / Software Engineering\n",
    "- **EffiCoder** (ICML 2025) **[Core]**  \n",
    "  https://icml.cc/virtual/2025/poster/46272\n",
    "- **Revisiting Chain-of-Thought in Code Generation** (ICML 2025) **[Intro/Core]**  \n",
    "  https://icml.cc/virtual/2025/poster/43621\n",
    "\n",
    "### Finance\n",
    "- **Advanced Financial Reasoning at Scale** (FinLLM @ IJCAI 2025, arXiv) **[Intro/Core]**  \n",
    "  https://arxiv.org/abs/2507.02954\n",
    "\n",
    "---\n",
    "\n",
    "# Reading Paths\n",
    "\n",
    "### Path 1: “High-Level SOTA Overview”\n",
    "1) RAG overview + faithfulness challenges **[Intro/Core]** (Conflicting evidence RAG)  \n",
    "2) Agents evaluation overview **[Intro/Core]** (IBM 2025 agent evaluation)  \n",
    "3) Flow/fast diffusion overview **[Core]** (SCoT NeurIPS 2025)\n",
    "\n",
    "### Path 2: “Generative Modeling Methods”\n",
    "1) Flow Matching theory gap-filling **[Advanced]** (ICML 2025 error analysis)  \n",
    "2) Rectified/consistency flows in vision **[Core]** (ICCV 2025 SR)  \n",
    "3) ICLR 2026 rectified flow for PDE surrogate modeling **[Advanced]** (OpenReview submission)\n",
    "\n",
    "### Path 3: “Efficient Small Models”\n",
    "1) KD for pretraining language models **[Core]** (ICLR 2025)  \n",
    "2) Contrastive distillation for LLMs **[Core]** (ICML 2025 DistiLLM-2)  \n",
    "3) Distilling *agents* into small models **[Core]** (NeurIPS 2025 Agent Distillation)\n",
    "\n",
    "### Path 4: “Domain-Specific GenAI”\n",
    "1) Clinical diagnosis model (AAAI 2025 MERA) **[Core]**  \n",
    "2) Medical time series foundation model (NeurIPS 2025 MIRA) **[Core]**  \n",
    "3) Financial reasoning benchmark (FinLLM@IJCAI 2025) **[Intro/Core]**\n",
    "\n",
    "---\n",
    "\n",
    "# Publishable Research Gaps (per topic)\n",
    "\n",
    "### 1) Fast Diffusion / Flow Matching\n",
    "- **Step-count vs. fidelity trade-offs** are still under-theorized for large-scale data (beyond toy settings).\n",
    "- **Calibration & uncertainty** for flow/diffusion outputs remains weak for safety-critical tasks.\n",
    "- **Cross-modality flows** (text+image+audio jointly) remain largely open.\n",
    "\n",
    "### 2) Distillation (student–teacher) for LLMs and Agents\n",
    "- Distilling *behavior* (tool use + long-horizon plans) is early; robust generalization is not well understood.\n",
    "- **Data selection/curricula** for KD (what to keep, what to drop) is not solved; “KD dataset engineering” is fertile.\n",
    "- **Distillation with guarantees** (faithfulness, safety, privacy) is still emerging.\n",
    "\n",
    "### 3) Retrieval-Native Generation (RAG)\n",
    "- RAG breaks under **conflicting or ambiguous evidence**; principled arbitration remains open.\n",
    "- **Multimodal RAG** evaluation is immature (ground-truth is hard; metrics lag).\n",
    "- **Citations and provenance** are not standardized across systems (open space for benchmarks).\n",
    "\n",
    "### 4) Agentic Systems\n",
    "- We lack **standardized, reliable eval** for memory, tool use, and long-horizon planning.\n",
    "- Agents often fail due to **compounding small errors**; error propagation analysis is publishable.\n",
    "- Safety for agents is harder than for chat: **action-space safety** is under-studied.\n",
    "\n",
    "### 5) Domain-Specific GenAI (Medicine/Finance/Code)\n",
    "- **Domain-safe training & evaluation** pipelines are not standardized (privacy, governance, liability).\n",
    "- Domain models need **uncertainty-aware** outputs and **abstention**; methods are fragmented.\n",
    "- Robustness across **institutions / jurisdictions / coding standards** is rarely tested.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb97797",
   "metadata": {},
   "source": [
    "\n",
    "## Writing Papers by Replication and Extension of Current Work\n",
    "\n",
    "The table below maps **key papers** to **replicable projects**, including datasets, baselines, and evaluation metrics. These can be  **publishable MSc / early PhD projects** if extended.\n",
    "\n",
    "| Research Theme | Representative Paper | Replication / Extension Idea | Dataset(s) | Baseline(s) | Evaluation Metrics |\n",
    "|---|---|---|---|---|---|\n",
    "| Flow Matching / Fast Diffusion | Lipman et al., *Flow Matching* (ICLR/NeurIPS) | Compare DDPM vs Flow Matching vs Rectified Flow at equal FLOPs | CIFAR-10, ImageNet-64 | DDPM, Consistency Models | FID, IS, NFE, wall-clock |\n",
    "| Hybrid AR–Latent Models | Rombach et al., *Latent Diffusion* | AR over latents vs pixel-space AR | ImageNet-64 | PixelCNN, LDM | FID, throughput |\n",
    "| Distillation (Student–Teacher) | Hinton et al.; DistiLLM-2 | Distill 7B → 1.3B with/without synthetic data | WikiText, OpenWebText | Teacher-only, student-only | Perplexity, task accuracy |\n",
    "| Agent Distillation | NeurIPS 2025 Agent Distillation | Distill tool-using agent into single model | GSM8K + tools | ReAct, Reflexion | Task success, steps |\n",
    "| Retrieval-Augmented Generation | Lewis et al., RAG | RAG vs no-RAG under conflicting evidence | HotpotQA | GPT-only | EM, citation accuracy |\n",
    "| Faithful Generation | Kang et al., AAAI 2024 | Train citation-aware decoder | SciFact | Seq2Seq | Faithfulness score |\n",
    "| Multimodal Reasoning | LLaVA-1.6 (ICML 2024) | CoT vs no-CoT in VLM reasoning | VQAv2 | LLaVA-base | Accuracy |\n",
    "| Small / Quantized LLMs | Phi-3 | INT8/INT4 trade-offs on edge GPUs | MMLU-lite | FP16 | Latency, accuracy |\n",
    "| Medical GenAI | Med-PaLM / MERA | Domain adaptation with uncertainty heads | MIMIC-III | General LLM | AUROC, ECE |\n",
    "| Code Generation | EffiCoder (ICML 2025) | Efficiency-aware LoRA vs full fine-tune | HumanEval | Code Llama | Pass@k |\n",
    "| Finance GenAI | FinGPT | Temporal drift analysis in financial text | FiQA | LLM baseline | F1, calibration |\n",
    "| Evaluation & Robustness | HELM | Stress-test under distribution shift | HELM tasks | Reported scores | Robustness delta |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff66187",
   "metadata": {},
   "source": [
    "\n",
    "## Various Levels of Research Project\n",
    "\n",
    "### Course-Level\n",
    "**Expectations:** correctness, clarity, reproducibility\n",
    "\n",
    "| Paper Anchor | Project Scope | Expected Contribution | Typical Venue Fit |\n",
    "|---|---|---|---|\n",
    "| Flow Matching (Lipman et al.) | Faithful re-implementation + hyperparameter study | Reproducibility + sanity checks | Course report |\n",
    "| RAG (Lewis et al.) | Compare RAG vs no-RAG on fixed dataset | Empirical confirmation | Workshop |\n",
    "| LLaVA | Multimodal ablation (CoT vs no-CoT) | Insightful analysis | Workshop |\n",
    "\n",
    "---\n",
    "\n",
    "### MSc Thesis\n",
    "**Expectation:** novelty via extension, strong evaluation\n",
    "\n",
    "| Paper Anchor | Project Scope | Expected Contribution | Typical Venue Fit |\n",
    "|---|---|---|---|\n",
    "| Rectified Flow | Speed–quality trade-off analysis | New empirical findings | NeurIPS/ICML Workshop |\n",
    "| Distillation (DistiLLM-2) | New distillation loss or curriculum | Method extension | AAAI / IJCAI |\n",
    "| Agent Distillation | Tool-use generalization study | New benchmark insight | NeurIPS Workshop |\n",
    "\n",
    "---\n",
    "\n",
    "### PhD Work\n",
    "**Expectations:** novelty, rigor, positioning\n",
    "\n",
    "| Paper Anchor | Project Scope | Expected Contribution | Typical Venue Fit |\n",
    "|---|---|---|---|\n",
    "| Hybrid AR–Latent–Flow | New architecture or theory | Architectural novelty | NeurIPS / ICML |\n",
    "| Faithful RAG | New faithfulness metric | Evaluation contribution | ACL / EMNLP |\n",
    "| Medical GenAI | Uncertainty-aware diagnosis | Safety + impact | AAAI / NeurIPS |\n",
    "\n",
    "---\n",
    "\n",
    "## General Expecatations\n",
    "\n",
    "| Criterion | What Reviewers Look For |\n",
    "|---|---|\n",
    "| Novelty | Clear delta over prior work |\n",
    "| Technical soundness | Correct math, justified design |\n",
    "| Evaluation | Strong baselines, ablations |\n",
    "| Reproducibility | Code, seeds, details |\n",
    "| Impact | Why this matters |\n",
    "| Limitations | Honest discussion |\n",
    "\n",
    "---\n",
    "\n",
    "## Course Project Grading Rubric\n",
    "\n",
    "| Component | Weight | Excellent (A) | Good (B) | Weak (C/F) |\n",
    "|---|---|---|---|---|\n",
    "| Problem formulation | 15% | Clear, well-motivated | Mostly clear | Vague |\n",
    "| Technical depth | 25% | Solid theory/implementation | Partial depth | Superficial |\n",
    "| Experimental design | 25% | Strong baselines & ablations | Limited ablations | Weak |\n",
    "| Analysis & insight | 20% | Deep, critical insights | Descriptive | Minimal |\n",
    "| Reproducibility | 10% | Fully reproducible | Partial | Not reproducible |\n",
    "| Writing & presentation | 5% | Clear, professional | Adequate | Poor |\n",
    "\n",
    "### Optional bonus (up to +5%)\n",
    "- Release code/data\n",
    "- Attempt workshop submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645861b",
   "metadata": {},
   "source": [
    "\n",
    "## Tying to Top Conferences like NeurIPS / ICML / AAAI Call-for-Papers Language\n",
    "\n",
    "This section maps **project contributions** directly to the **language used in NeurIPS, ICML, and AAAI CFPs**.  \n",
    "You can use this phrasing *verbatim* when framing abstracts, introductions, and contributions.\n",
    "\n",
    "---\n",
    "\n",
    "## NeurIPS\n",
    "**CFP emphasis:** *Novel algorithms, theoretical insights, strong empirical evaluation, broad impact*\n",
    "\n",
    "### What NeurIPS reviewers expect\n",
    "- Clear **algorithmic novelty** or **new theoretical insight**\n",
    "- Rigorous experiments with **strong baselines**\n",
    "- Discussion of **limitations and broader impacts**\n",
    "- Reproducibility checklist compliance\n",
    "\n",
    "### Aligned project examples\n",
    "- **Hybrid AR–Latent–Flow model**  \n",
    "  *“We propose a hybrid generative architecture that unifies autoregressive reasoning with latent flow-based decoding, achieving improved efficiency–quality trade-offs.”*\n",
    "- **Fast diffusion via flow matching**  \n",
    "  *“We introduce an empirical and theoretical analysis of step-count vs fidelity in rectified flow models.”*\n",
    "- **Agent distillation**  \n",
    "  *“We study whether complex tool-using behaviors can be distilled into compact policies without loss of task performance.”*\n",
    "\n",
    "**Typical contribution statement:**  \n",
    "> *We introduce a new method / analysis / benchmark and demonstrate consistent improvements across multiple datasets.*\n",
    "\n",
    "---\n",
    "\n",
    "## ICML\n",
    "**CFP emphasis:** *Sound methodology, learning principles, careful ablation, generalization*\n",
    "\n",
    "### What ICML reviewers expect\n",
    "- Method grounded in **learning theory or optimization**\n",
    "- Extensive **ablation studies**\n",
    "- Clear explanation of *why* the method works\n",
    "- Clean, well-controlled experiments\n",
    "\n",
    "### Aligned project examples\n",
    "- **Student–teacher distillation curriculum**  \n",
    "  *“We analyze curriculum-aware distillation strategies and show improved generalization in compact language models.”*\n",
    "- **Quantization-aware generative modeling**  \n",
    "  *“We investigate how low-bit quantization alters optimization dynamics and representational capacity.”*\n",
    "- **RAG under distribution shift**  \n",
    "  *“We systematically study retrieval-augmented generation under conflicting or noisy evidence.”*\n",
    "\n",
    "**Typical contribution statement:**  \n",
    "> *We present a principled learning approach and validate it through controlled empirical studies.*\n",
    "\n",
    "---\n",
    "\n",
    "## AAAI\n",
    "**CFP emphasis:** *Practical relevance, robustness, evaluation, societal impact*\n",
    "\n",
    "### What AAAI reviewers expect\n",
    "- Clear **application motivation**\n",
    "- Robustness, safety, or interpretability angle\n",
    "- Comprehensive evaluation on **realistic datasets**\n",
    "- Explicit discussion of **limitations and ethics**\n",
    "\n",
    "### Aligned project examples\n",
    "- **Faithful and cited RAG systems**  \n",
    "  *“We propose a citation-aware decoding strategy that improves factual faithfulness in knowledge-intensive tasks.”*\n",
    "- **Medical generative models with uncertainty**  \n",
    "  *“We design uncertainty-aware generative models for clinical decision support.”*\n",
    "- **Efficient domain-specific LLMs**  \n",
    "  *“We demonstrate that small, distilled models can outperform larger general models in regulated domains.”*\n",
    "\n",
    "**Typical contribution statement:**  \n",
    "> *We demonstrate a robust and practical AI system validated on real-world data.*\n",
    "\n",
    "---\n",
    "\n",
    "## Cross-Venue Framing Cheat Sheet\n",
    "\n",
    "| If your contribution is mainly… | Frame it like NeurIPS | Frame it like ICML | Frame it like AAAI |\n",
    "|---|---|---|---|\n",
    "| New architecture | Algorithmic novelty | Learning dynamics | Practical gains |\n",
    "| New loss / objective | Optimization insight | Theory + ablation | Robustness |\n",
    "| Empirical study | Broad benchmark | Controlled analysis | Realistic scenarios |\n",
    "| Domain application | Impact discussion | Generalization | Societal relevance |\n",
    "| Evaluation method | Benchmark contribution | Measurement validity | Reliability |\n",
    "\n",
    "---\n",
    "\n",
    "## General Framing Advice\n",
    "- **Same project → different venue framing**\n",
    "- NeurIPS: *“What is new?”*  \n",
    "- ICML: *“Why does it work?”*  \n",
    "- AAAI: *“Why does it matter in practice?”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ccaa6",
   "metadata": {},
   "source": [
    "\n",
    "## Mapping Projects to Workshops (NeurIPS / ICML / AAAI / ICLR)\n",
    "\n",
    "This section maps **project types** to **realistic workshops**, using language and scope aligned with how workshops are actually pitched and reviewed.\n",
    "\n",
    "---\n",
    "\n",
    "## NeurIPS Workshops\n",
    "\n",
    "| Project Theme | Suitable Workshop(s) | Why This Fits |\n",
    "|---|---|---|\n",
    "| Fast diffusion / flow matching | *Workshop on Score-Based & Diffusion Models* | Focus on efficiency, sampling, and theory |\n",
    "| Hybrid AR–latent models | *Workshop on Multimodal Learning* | Cross-modal architectures and representations |\n",
    "| Agent distillation | *Foundation Models for Decision Making* | Planning, tools, and agent behavior |\n",
    "| Evaluation & robustness | *Benchmarking and Evaluation of Foundation Models* | Metrics, stress tests, failure modes |\n",
    "| Small / efficient models | *Efficient Natural Language and Vision Processing* | Compute-aware modeling |\n",
    "\n",
    "---\n",
    "\n",
    "## ICML Workshops\n",
    "\n",
    "| Project Theme | Suitable Workshop(s) | Why This Fits |\n",
    "|---|---|---|\n",
    "| Distillation methods | *Workshop on Knowledge Distillation* | Learning principles and compression |\n",
    "| Optimization-aware quantization | *Efficient Deep Learning* | Training/inference trade-offs |\n",
    "| Learning dynamics of flows | *Implicit Models and Optimization* | Theory-driven contributions |\n",
    "| Synthetic data for LLMs | *Data-Centric Machine Learning* | Dataset design and curricula |\n",
    "\n",
    "---\n",
    "\n",
    "## AAAI Workshops\n",
    "\n",
    "| Project Theme | Suitable Workshop(s) | Why This Fits |\n",
    "|---|---|---|\n",
    "| Faithful RAG | *Workshop on Trustworthy AI* | Reliability, citations, ethics |\n",
    "| Medical GenAI | *AI in Healthcare* | Practical impact and safety |\n",
    "| Domain-specific LLMs | *Applied AI for Industry* | Real-world deployment |\n",
    "| Agent safety | *AI Safety and Governance* | Risk, misuse, safeguards |\n",
    "\n",
    "---\n",
    "\n",
    "## ICLR Workshops (Early-Stage / Risky Ideas)\n",
    "\n",
    "| Project Theme | Suitable Workshop(s) | Why This Fits |\n",
    "|---|---|---|\n",
    "| New generative objectives | *Workshop on New Frontiers in Representation Learning* | High-risk, high-reward ideas |\n",
    "| Flow–RL connections | *Bridging Deep Learning and Control* | Conceptual unification |\n",
    "| Theory of distillation | *Theory of Deep Learning* | Mathematical grounding |\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Mapping\n",
    "\n",
    "- **Course project** → ICML / AAAI workshop  \n",
    "- **Strong MSc thesis** → NeurIPS / ICML workshop  \n",
    "- **Early PhD idea** → ICLR workshop (feedback-first)  \n",
    "- **Mature result** → main conference track\n",
    "\n",
    "---\n",
    "\n",
    "## Example Framing (Workshop Abstract Sentence)\n",
    "> *“This paper presents an empirical study of X, highlighting limitations and open questions that motivate future research.”*\n",
    "\n",
    "This framing is often more effective for workshops than “we beat all baselines.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use of Generative AI\n",
    "\n",
    "Portions of this material were developed with the assistance of **generative artificial intelligence tools.  \n",
    "The author reviewed, edited, and validated all content, including explanations, code, and examples, and assumes full responsibility for accuracy and interpretation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
